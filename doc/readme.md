# 算法详述
## 灯条检测
这一部分沿用了大疆开源的代码，核心思想就是灯条中心的亮度很高，但是颜色分量不足。颜色分量主要集中于灯条的周围。所以需要综合 color 和 light 两个方面做检测。
其中形态学可有可无: 膨胀 or 开运算 (对快递移动的情况可能有影响，装甲识别有很多经验参数是一帧一帧调出来的)
```c++
  // src/detect_factory/armor_detect.cpp：line 165
	cv::dilate(binary_color_img, binary_color_img, element, cv::Point(-1, -1), 1);
	binary_light_img = binary_color_img & binary_brightness_img;
```
## 对灯条进行一次筛选
这里的主要目的是把灯柱血条等错误的识别筛掉，但是移动过程中，Armor会有很严重的拖影，按照最理想的情况进行筛选，镜头一旦晃动可能就啥都看不到了。（我们用的摄像头不到两百，跟工业相机确实无法比），所以筛选的条件不能太过于严格。
1. adjustrect 调整灯条的角度和与长宽
2. 针对高速移动带有拖影的灯条进行筛选
3. 针对灯条细小的情况，较为理想进行筛选。包括：
   - 根据灯条的数值角度
   - 根据灯条自身的纵横比例
   - 根据灯条的最大面积和最小面积

## 匹配Armor
感谢大疆开源的图像预处理，使得灯条可以非常干净的被提取出来。但也有很多难以克服的地方：
太依赖相机的曝光设置了，对环境光线太敏感，这个问题可以通过临场调试来解决，但是确实不优雅。在这份代码里，曝光设置在`calibration-param/`下的相机配置文件里，这部分代码在`src/driver/camera_driver.cpp`
```c++
capture_camera_forward.set(CV_CAP_PROP_FOURCC,CV_FOURCC('M','J','P','G'));
capture_camera_forward.set(CV_CAP_PROP_FRAME_WIDTH, 640);
capture_camera_forward.set(CV_CAP_PROP_FRAME_HEIGHT, 480);
set_camera_exposure(usb_cam_id, exposure_time);  // 关于usb_cam_id，可以参考 doc/80-usbcam.rules
```
在参考了历届算法开源之后，我们重写了后续的算法流程，首先是灯条配对成Armor的一些条件：
1. 左右灯条的角度数值差（举例说明：灯条偏左5度就是-5，偏右5度就是+5）
2. Armor矩形的垂直角度与左右灯条的数值角度的绝对值积累差（接着上面的举例，如果框选的Armor恰好完全水平，这个值就等于10；这个值可以衡量原本的两个灯条和现在框出来的矩形有多拟合）
3. 矩形armor的长宽比例，这一数值需要根据灯条的角度差来进行调整，因为俯仰角会影响比例
4. 左右灯条的高度差与宽度差（这个参数已经不那么重要了，甚至会降低识别了，面对远距离，可能其中一个灯条只有另一个灯条长度的1/5）
5. 灯条自身的比例（灯条细小的情况不适合用这个标准来衡量，比如当Armor侧着面对你，较远的灯条在检测中总是更细）
6. 包围矩形要取 max_height 与 min_width 可以更好固定比例，也包括 max_area 与 min_area，这些都是辅助的标准，最主要的只有前两条。

针对镜头移动的拖影，我们单独考虑了很多情况：
1. Armor快速移动，两灯条平行
2. 同上，其中一块略有倾斜
3. 针对中速拖影进行处理、平行、90度平行、带有误差的平行[待补充]
4. 灯条几乎同侧的匹配
5. 灯条不同侧的误差匹配
6. 其他边界情况

在分类讨论的过程中，我们发现可以简单地对Armor速度分级，比如两个几乎正方形甚至两个平躺的长方形匹配成一个Armor我们就认为它是中速移动甚至快速移动的。检测Armor的移动速度对后面的预测等工作都很有帮助。这部分写成了一个枚举：
```c++
include/detect_factory/armor_info.hpp：line 92
enum Armor_Twist { STILL = 1, LOW_MOVE = 2, MID_MOVE = 3, FAST_MOVE = 4 }; // 速度信息
```
extra/armor_sample中的流程就只包含以上的部分，会识别很多Armor（正样本），也会识别很多两块Armor之间（负样本）。

## 筛选Armor
1. 进行一次去重操作，如果两个Armor重叠在一起，就返回面积更大的那一块。这里的逻辑非常简单，零耗时。
2. 计算两个装甲板是否接近于紧挨着的，如果是
3. 对比Armor的长宽比例，有显著区别就剔除比例更大的那一块（侧对着机器人，前面和侧面之间的空档在匹配Armor的条件2就会剔除掉，这里针对的是一辆车完全侧对着你，不能根据匹配Armor的条件2筛到的情况做补充），如果无法判断就
4. 对比Armor的水平角度，有显著区别就保留更水平的Armor，这里的参数很可能导致一辆车正对着你，但是算法框选了正面Armor两侧的装甲板（哨兵视野尤其容易出现这个问题，因为是俯视），如果无法判断就
5. 对比Armor的面积，有一块面积小于另一块100像素以上就保留小的，其实到这一步已经很难以分辨了，如果已经到了无法分辨的程度
6. 框选Armor的Roi区域，感谢官方提供的贴纸，面积太小的不计算，会影响远距离的识别。
   - 计算mean与stddev，这里受光照影响很大，针对不同速度移动要分类讨论，因为一动，中间的数字完全看不清楚。
   - 计算颜色直方图，可以处理成灰度图，提高速度。
   - 用SVM分类器，速度还可以接受，大装甲Size(100,25)，小装甲Size(60,25)，为了提高速度，应该可以进一步缩小Roi的尺寸。

针对Roi的三种计算，代码都保留了，最终调用的是SVM。我们先根据长宽比例粗暴判断了是大装甲还是小装甲，然后套不同的SVM。这里的做法非常不靠谱，这份代码判断大小装甲都是依据长宽比例，错误率很客观，因为装甲板会侧对，而且哨兵视角会俯视。
我们建议的改进是套SVM进行更细化的分类，不仅可以区别大小装甲，还能区别每一辆车12345，SVM的速度并不慢，但是每一帧都套SVM就有点儿慢了。

## 击打Armor策略
之前的计算完整走下来，还是有可能有多个Armor，我们对所有的Armor做PnP解算，然后计算Pos距离：
```c++
// include/detect_factory/armor_info.h
struct armor_pos
{
	int Flag;        // 标志位
	double angle_x;  // Yaw角度
	double angle_y;  // Ptich角度
	double angle_z;  // 距离信息
}；
// src/slover/armor_recorder.cpp
inline bool pos_distance(const vision_mul::armor_pos& pos1, const vision_mul::armor_pos& last_pos )
{
    return 
    std::sqrt((pos1.angle_x - last_pos.angle_x) * (pos1.angle_x - last_pos.angle_x) + 
              (pos1.angle_y - last_pos.angle_y) * (pos1.angle_y - last_pos.angle_y) +
               std::abs(pos1.angle_z - last_pos.angle_z))/10; // 距离数值太大了，除以10是为了距离影响 
}
```
有一个armor_recorder类会记录各种先验的信息（上一帧的七大姑八大姨），我们是为了让云台变懒，每次只挪动最小的角度打离我最近的Armor，假如有个敌人的车从面前横穿过去也不要干扰识别，继续非常懒地打之前打的目标。

这里有非常大的优化空间，我们这个策略也不够好。可以考虑：
- 加领域搜索（不好和当前逻辑整合）
- 连续识别多少次再开始射击（这会导致串口发送率极其低下）
- 连续几帧失去目标在开始巴拉巴拉
- 融合两帧的时间做预测，融合之前的速度信息做预测（我们处理不好预测算法，索性去掉）

## 串口通信
我们假设串口通信是没有延迟的，串口进行双向通信：
```c++
unsigned char data[7] = {0xDA,
                         0x00,0x00,  // Yaw
                         0x00,0x00,  // Pitch
                         0xDB};

unsigned char send_bytes[] = { 0xFF,        // 头
                               0x00,        // Flag
                               0x00,0x00,   // Yaw
                               0x00,0x00,   // Pitch
                               0x00,        // 距离
                               0xFE};       // 尾
```
电控给视觉发送云台陀螺仪当前的角度，视觉给电控发送云台需要转到的角度（而不是需要转多少度）。

这里也有很大的优化空间，因为电控陀螺仪的数据在图像处理中并没有用到，尤其是在处理拖影的时候，拖影是由镜头移动或者对方的车移动产生的，完全可以整合陀螺仪进去（那样视觉算法和陀螺仪耦合程度会很高，不过值得一试）。

## 未来的改进
1. 如果发现写不下去了，可能会用ROS。当初是打算用ROS的，但是后来所有的代码中要解决的问题都解决了，就没有用ROS，我们的代码环境相当简单。
2. 视觉算法和陀螺仪数据会耦合在一起，陀螺仪动，则算法参数动，移动情况下的亮度会变暗、会有拖影。太多的几何参数（比例、长宽、角度）都需要建一个模型。
3. 继续 improve detect 是一方面，我们还没有攻克预测算法，为了打死快速移动的机器人，预测算法必不可少。
4. 大疆的“亮度在灯条中心、颜色在灯条周围”预处理算法效果太好了（在曝光合适的时候），我们想探索自己的图像处理方法，目前的代码几乎没有创新，这也是我们最大的不足。
